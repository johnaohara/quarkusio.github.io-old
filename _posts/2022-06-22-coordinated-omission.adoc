---
layout: post
date:   2022-06-23 00:00 +0100
author: johara
tags: performance
title: "What is Coordinated Omission?"
synopsis: 
---
:imagesdir: /assets/images/posts/coordinated-omission

Learn about Coordinated Omission. What is Coordinated Omission? How does it effect benchmark results and how can we design a test that does not suffer from coordinated omission.

== Tl;dr

Coordinated Omission occurs when the load generator we choose is not able to accurately create a workload representative of real world traffic when load testing a Web service. 

There is a "Coordination" from the System Under Test applying indirect back pressure to the load driver, that causes the load driver to "Omit" any number of valid results.

Response time metrics measured with tools that suffer from Coordinated Omission are far from misleading, they are wrong. The worst part is, the load generator can not detect or inform users that the results are incorrect.

Using tools such as https://hyperfoil.io/[Hyperfoil], you can be sure that any response time metrics it captures are accurate; or, if it detects back-pressure from the System Under Test, it will record and report the unintended back-pressure.

== Relevant Articles

Coordinated Omission is a term that has been in circulation for a while now and there are many articles that describe

For more information, please visit the following articles;

- http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html



== Tell me about this Coordinated Omission thingy!

Before exploring the nuances of coordinated omission, lets get clear in our minds what it is we are trying to do when we are measuring system "response time".

Imagine a scenario where we run a call center for a building materials trade counter (*Building Star Trade Supplies Inc*)

Our tag line is "*_Building Star Trade Supplies Inc: the best in the world for all your building supply needs!_*"

To improve customer experience, we want to reduce the time it takes for our customers to place an order. The faster customers can place an order, the happier they are, and the faster we can build a world beating business!

What do we need to do; measure how long it takes to place a telephone order through our switchboard.

If it takes longer than 3 minutes to place an order we will start to loose market share to our rivals _Prime Materials Supplies_. 

=== A typical interaction

image::customer-interaction.png[Interaction,400,400,float="right",align="center"]

Typically a customer calls the call center, is put through to an operator, where the operator checks stock levels, raises a new order and confirms with the customer before terminating tha call.

=== How long does it take to place an order?

image::coordinated-omission-placeOrder.png[Order timeline]

There are 2 main components in the time taken to place the order;  

* the *"Wait Time"* the customer was being held in the switchboard queue before being put through the operator
* The *"Service Time"* it took for the operator to process the customer request

From the customers point-of-view, the total *"Order Time"* (i.e. the time taken to place the order) was ;

[IMPORTANT]
====
*"Order Time" = "Wait Time" + "Service Time"*

To measure the performance of our call center, we need to measure the total "Order Time".

If it only takes a single operator 30 seconds to process an order request, but customers are waiting on average 30 minutes to be connected to them, the customer experience is poor"
====

== "What does a call center have to do with my Web Service?!?"

There are a lot of similarities between a call center and a Web Service. 

[NOTE]
====
Any number of _customers_ (*clients*) can _call_ (*tcp_socket::open*) our _switchboard_ (*web service*), where an _operator_ (*thread*) will process the _enquiry_ (*request*), interacting with multiple _systems_ (*backend services*) before confirming an _order_ (*response*) and terminating the _call_ (*tcp_socket::close*). 
====

Typically for us developers of web services; before we push our changes into production, we want to know how it will scale or perform under sustained client load. So, we build a benchmark, take some measurements and deploy depending on if we see acceptable performance.

But *how* do we measure performance of a web service? What decisions do we take, and how does this impact our confidence in how our web service performs. 

== Measuring System performance

So, you've been tasked with ensuring the application can handle production workload. The checklist typically looks something like;

* [*] pick a load generation tool
* [*] setup benchmarking environment
* [*] run load generation tool against test environment
* [x] ensure the system response times are within required SLA's
* [ ] ship it!

[IMPORTANT]
====
*The first step is crucial! : Picking a load generation tool that models reality is vital in shipping a product that behaves they way you expecct it to*
====

== Modelling the real world

Let's go back to our *Building Star Trade Supplies Inc*. In order to improve customer experience, we have built a brand new call center *BSTSI-callHandler-2.0*!! 

We have tested to make sure the call center works (*functional testing*); but before we start taking customers calls there, we need to ensure that it is more efficient (*load testing*) than *BSTSI-callHandler-0.1.BETA*.

For this work, we need to design a test that models the real world!

=== Designing the "load" test

Our SLA for our call center stipulates that we need to be able to;

- Process *20 orders per minute*
- Customers must spend *on average less than 1 minutes* on the phone placing an order.
- 99% of customers should be able to place an order within *3 minutes*

So, lets bring in a number of dummy customers (*clients*) that will ring the call center and place fictitious orders. We can measure how long it takes for each dummy customer to place and order. 

To meet the SLA, we need to be able to process *20 orders per minute* (*throughput*) with the average telephone call taking less than *3 minutes* (*response time*).

=== First attempt

image::firstTest.png[First Test,400,400,float="right",align="center"]

. Each tester is given their own phone and a list of orders to place

. The tester calls the new call center and places an order

. After the call terminates, the tester checks the phone screen to see how the call took, and records this as the "Order Time"

. After all the testers have run through their list of dummy orders. The call times for all the testers are collated 

After reviewing the call logs, we found the average call duration was *1min 20secs*

=== Ship It!!

image::realworld.png[Real World, 400, 400,float="right",align="center"]

Fairly soon after opening *BSTSI-callHandler-2.0*, negative reviews start appearing.  Some people are frustrated that they can not quickly get through to place an order. And it is not just one person! there are a few disgruntled customers.


=== What went wrong?

*Our test had some fundamental flaws*. While everything appeared to provide us with the data to give us confidence about how the call center would perform, we were not quite modelling how calls would arrive *in the real world*.

Each tester had been given their own phone and a list of orders to place. This puts some limits on the test;
[IMPORTANT]
====
. *Testers can only place one order at a time!*

. *Testers are blocked* from placing any more orders until their current order is placed.  

. In the real world there are many more customers, *each with their own phone*

. In the real world, *customers enquiries arrive at different, random times, often in parallel*. 
====

In addition to to the problems above, In the real world, there are *hiccups*.  For example, in our call center there are Lunch breaks, fire alarms, computer systems crash.

=== How does a Hiccup effect our call center performance?

We have our imaginary call center, now lets have an imaginary outage! 

Lets say we were making 1,000 new orders, with 10 testers.  Each order takes on average 30 seconds to complete, so the call center should be handling 20 calls per minutes (2 per tester).

Someone has planned an upgrade to the backend systems at 1:15pm, which should not impact business, but it knocks all the backend systems out for 10 minutes, tying up the operators until the backend systems come back online.

=== Lets do some Math!

What does this do the to summary statistics? A 10 minute Hiccup in our call center while testing would have been observed like this;

image::coordinated-omission-blocked-wait-time.png[Blocked Wait Time]

Whereas, *in reality*, a 10 minutes Hiccup will be observed like this by our customers;

image::coordinated-omission-cumulative-wait-time.png[Cumulative Wait Time]

If you do not want to get bogged down by the maths behind this, skip to <<Why are the numbers so different?>>

10 testers had a call each that lasted 930 seconds (15min delay + 30second service time). No other calls were received during that 5 minute period because all the testers we blocked from making more calls. 

Therefore the average call duration was 990 * 30 + 10 * 930 / 1000 ; *39 seconds*

The 99th centile would have been measured as: 30 seconds

But, in the real world, with 1,000 *different* customers, the average experience would have been *alot* different.

If we received 20 calls per minute, with calls blocked for 15 minutes, we would have experienced at least 20 * 5 = *100* calls taking 930 seconds. Possibly even more as the backlog gets cleared.

The actual average call duration would have been 900 * 30 + 100 * 930 / 1000 ; *120 seconds*

The 99th centile would have been measured as: 930 seconds!

Our testing showed an average response time of *39 seconds*, whereas in reality it *would* have been *120 seconds*


Our test methodology gave an average order time of 39 seconds, and 99% of customers were serviced in 30 seconds.  In reality, with actual real customers, the average call time was 120 seconds, and the the 99th centile was 930 seconds (15.5 minutes)




=== Why are the numbers so different?

[IMPORTANT]
====
In our testing scenario, there were 10 testers who were blocked for 15 minutes. In reality users would *keep calling the telephone line* and being put on hold until an operator became available.

*We have not captured the waiting time of 200 of our users who would have waited up to 15 minutes in the queue*
====

=== Second Attempt

. Each tester is given *multiple phones* and a list of orders to place *and the time to start the call*

. The tester(s) calls the new call center to place orders

. If the first call does not complete before the next order needs to be placed, the tester users a *different* phone to make the next call. Any one tester can have multiple calls *running concurrently* at the same time.

. After the call terminates, the tester checks the phone screen to see how the call took, and records this as the "Order Time"

. After all the testers have run through their list of dummy orders. The call times for all the testers are collated and summary data is calculated.

. If the tester runs out of phones, they record how long they did not have any more phones available for.  The sum of this time is the total *blocked time* during the load test.  This is a direct measurement of Coordinated Omission.

=== What is different?

In the first test, the testers could only make one call and made the calls in sequence. 

If there was a hiccup, they could not start a new call.  

== What can be done?

A load generation tool that uses asynchronous I/O and uncouples threading from I/O, which measures timing independent on I/O is able to detect when the System Under Test is applying back-pressure to the load generator.

Tools such as Hyperfoil will detect *and report* backpressure, so you *can* be sure that the load generator is reporting accurate response times without any Coordinated Omission effects from the SUT.

== But surely, if I have enough threads, I can tune away this problem?

Unfortunately not, you *might* be able to mitigate some of the issues through tuning, but you can *never be certain that the results are accurate*. The fundamental problem is that there is *missing data*.

Statements are often made such as _'As with any Load Testing tool, if you don't correctly size the number of threads, you will face the "Coordinated Omission" problem which can give you wrong or inaccurate results.'_ (https://jmeter.apache.org/usermanual/best-practices.html)

The fundamental issue is not with the _size of the thread pool_, but whether the load generator threads that measure response time _can be blocked by the System Under Test_.  

[IMPORTANT]
====
*If the load generation tool does not decouple time measurement from generating load, the problem can not be tuned away.*
====

== Summary

